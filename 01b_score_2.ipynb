{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.layers import Dense, Dropout, GRU, Embedding \n",
    "from keras.layers import Input, Activation, concatenate, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils, get_custom_objects\n",
    "from keras.preprocessing import text, sequence\n",
    "from string import ascii_letters, punctuation, digits\n",
    "\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "SEQ_LENGTH = 100\n",
    "EMBED_SIZE = 300\n",
    "VOCAB = 238590\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(matrix):\n",
    "    rnn = {}\n",
    "    rnn['units'] = 75\n",
    "    rnn['return_sequences'] = True\n",
    "    rnn['recurrent_dropout'] = 0.2\n",
    "    rnn['dropout'] = 0.1\n",
    "    rnn['activation'] = 'tanh'\n",
    "    inputs = Input(shape=(SEQ_LENGTH,), name='sequence')\n",
    "    embed = Embedding(VOCAB,EMBED_SIZE, weights=[matrix], trainable=False)(inputs)\n",
    "    lstm = Bidirectional(GRU(**rnn))(embed)\n",
    "    lstm = BatchNormalization()(lstm)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm)\n",
    "    pool = concatenate([max_pool, avg_pool])\n",
    "    pool = BatchNormalization()(pool)\n",
    "    lstm = Dropout(0.2)(pool)\n",
    "    dense = Dense(200, activation='swish')(lstm)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    predict = Dense(6, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=[inputs], output=predict)\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('../data/download/glove.6B.300d.txt')\n",
    "\n",
    "for line in f:\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(values[0])\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataflow(train_text, valid_text, score_text):\n",
    "    train_text['comment_text'] = train_text['comment_text'].fillna('nan')\n",
    "    valid_text['comment_text'] = valid_text['comment_text'].fillna('nan')\n",
    "    score_text['comment_text'] = score_text['comment_text'].fillna('nan')\n",
    "    train_text = list(train_text['comment_text'].values)\n",
    "    valid_text = list(valid_text['comment_text'].values)\n",
    "    score_text = list(score_text['comment_text'].values)\n",
    "    tokenizer = text.Tokenizer(lower=True, char_level=False, num_words=20000)\n",
    "    tokenizer.fit_on_texts(train_text + valid_text)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('total words:', len(word_index))\n",
    "    intersect = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            intersect += 1\n",
    "    print('common words:', intersect)\n",
    "    score_token = tokenizer.texts_to_sequences(score_text)\n",
    "    score_seq = sequence.pad_sequences(score_token, maxlen=SEQ_LENGTH)\n",
    "    return score_seq, embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(mode):\n",
    "    train_text = pd.read_csv('../data/data/source_1/train/train_data_{}.csv'.format(mode))\n",
    "    valid_text = pd.read_csv('../data/data/source_1/train/test_data_{}.csv'.format(mode))\n",
    "    score_text = pd.read_csv('../data/data/source_1/score/score_text.csv')\n",
    "    score_data = score_text[['id']]\n",
    "    score_text, embedding_matrix = dataflow(train_text, valid_text, score_text)\n",
    "    model = define_model(embedding_matrix)\n",
    "    path = '../data/data/source_1/model_2/model_{}.hdf5'.format(mode)\n",
    "    model.load_weights(path)\n",
    "    scores = model.predict(score_text, batch_size=512)\n",
    "    scores = pd.DataFrame(scores)\n",
    "    scores.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "    scores = score_data.join(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 238589\n",
      "common words: 94353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 238589\n",
      "common words: 94353\n",
      "total words: 238589\n",
      "common words: 94353\n",
      "total words: 238589\n",
      "common words: 94353\n",
      "total words: 238589\n",
      "common words: 94353\n"
     ]
    }
   ],
   "source": [
    "score_1 = score_model(1)\n",
    "score_2 = score_model(2)\n",
    "score_3 = score_model(3)\n",
    "score_4 = score_model(4)\n",
    "score_5 = score_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.985434</td>\n",
       "      <td>0.443933</td>\n",
       "      <td>0.935825</td>\n",
       "      <td>1.125207e-01</td>\n",
       "      <td>0.888198</td>\n",
       "      <td>0.538948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>2.193530e-08</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>9.460520e-07</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>1.221449e-05</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>4.463878e-05</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
       "0  00001cee341fdb12  0.985434      0.443933  0.935825  1.125207e-01  0.888198   \n",
       "1  0000247867823ef7  0.000093      0.000001  0.000042  2.193530e-08  0.000026   \n",
       "2  00013b17ad220c46  0.001082      0.000077  0.001127  9.460520e-07  0.000357   \n",
       "3  00017563c3f7919a  0.000262      0.000004  0.000129  1.221449e-05  0.000150   \n",
       "4  00017695ad8997eb  0.003484      0.000131  0.001249  4.463878e-05  0.000377   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.538948  \n",
       "1       0.000001  \n",
       "2       0.000099  \n",
       "3       0.000018  \n",
       "4       0.000049  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = score_1.append(score_2)\n",
    "submit = submit.append(score_3)\n",
    "submit = submit.append(score_4)\n",
    "submit = submit.append(score_5)\n",
    "submit = submit.groupby('id').mean().reset_index()\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../data/submit/baseline_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB - 0.9805"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
