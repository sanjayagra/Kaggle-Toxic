{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from dask.multiprocessing import get\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import datefinder\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "stm = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dates(text):\n",
    "    dates = list(datefinder.find_dates(text, source=True))\n",
    "    dates = [x[1] for x in dates]\n",
    "    for date in dates:\n",
    "        text = text.replace(date,'')\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    ipaddress = re.findall( r'[0-9]+(?:\\.[0-9]+){3}', text)\n",
    "    for ip in ipaddress:\n",
    "        text = text.replace(ip,'')\n",
    "    text = clean_dates(text)\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"havn't\", \"have not\")\n",
    "    text = text.replace(\"n't\",\" not\")\n",
    "    text = text.replace(\"i'm\", \"i am\")\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"there's\", \"there is\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    text = text.replace(\"e-mail\", \"email\")\n",
    "    text = text.replace(\"you'll\", \"you will\")\n",
    "    text = re.sub('([' + string.punctuation + '“”¨«»®´·º½¾¿¡§£₤‘’])', '', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = ' '.join([x.strip() for x in text])\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal data: (312735, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/download/train.csv', usecols=['comment_text'])\n",
    "test_data = pd.read_csv('../data/download/test.csv', usecols=['comment_text'])\n",
    "internal = train_data.append(test_data)\n",
    "internal['comment_text'] = internal['comment_text'].fillna('nan')\n",
    "internal = dd.from_pandas(internal, npartitions=10)\n",
    "internal = internal.map_partitions(lambda df: df.apply((lambda row: clean_text(*row)),axis=1))\n",
    "internal = internal.compute(get=get)\n",
    "internal = pd.DataFrame(internal, columns=['comment_text'])\n",
    "del train_data, test_data\n",
    "print('internal data:', internal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_1 = pd.read_csv('../data/download/attack_annotated_comments.tsv', sep='\\t')\n",
    "external_1['comment'] = external_1['comment'].map(lambda x : x.replace('NEWLINE_TOKEN',' '))\n",
    "external_2 = pd.read_csv('../data/download/aggression_annotated_comments.tsv', sep='\\t')\n",
    "external_2['comment'] = external_2['comment'].map(lambda x : x.replace('NEWLINE_TOKEN',' '))\n",
    "external_3 = pd.read_csv('../data/download/toxicity_annotated_comments.tsv', sep='\\t')\n",
    "external_3['comment'] = external_3['comment'].map(lambda x : x.replace('NEWLINE_TOKEN',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "external data: (391414, 1)\n"
     ]
    }
   ],
   "source": [
    "external = external_1[['comment']].copy()\n",
    "external = external.append(external_2[['comment']])\n",
    "external = external.append(external_3[['comment']])\n",
    "external = external.rename(columns={'comment':'comment_text'})\n",
    "del external_1, external_2, external_3\n",
    "external = dd.from_pandas(external, npartitions=10)\n",
    "external = external.map_partitions(lambda df: df.apply((lambda row: clean_text(*row)),axis=1))\n",
    "external = external.compute(get=get)\n",
    "external = pd.DataFrame(external, columns=['comment_text'])\n",
    "print('external data:', external.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = internal.append(external)\n",
    "data['comment_text'] = data['comment_text'].fillna('nan')\n",
    "data = data.sample(frac=1.0, random_state=2017)\n",
    "data[['comment_text']].to_csv('../data/data/fasttext/data.txt', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 46M words\n",
      "Number of words:  93650\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  103683 lr:  0.000000 loss:  1.320773 ETA:   0h 0m 0.023224 loss:  1.773688 ETA:   0h 1m 0.019452 loss:  1.749323 ETA:   0h 1mh 0m 1.536813 ETA:   0h 0m\n"
     ]
    }
   ],
   "source": [
    "!fasttext skipgram -input ../data/data/fasttext/data.txt -output ../data/data/fasttext/vector -minCount 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
