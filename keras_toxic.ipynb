{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input,GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 8) (226998, 2)\n"
     ]
    }
   ],
   "source": [
    "path = '/axp/rim/imsadsml/warehouse/sagra39/Kaggle/toxic/'\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Nonsense?  kiss off, geek. what I said is true...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"unknown\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"unknown\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.\",\n",
       "       '\"\\n\\n Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.    \"',\n",
       "       '\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the \"\"points of interest\"\" section you added because it seemed kind of spammy. I know you probably didn\\'t mean to disobey the rules, but generally, a point of interest tends to be rather touristy, and quite irrelevant to an area culture. That\\'s just my opinion, though.\\n\\nIf you want to reply, just put your reply here and add {{talkback|Jamiegraham08}} on my talkpage.   \"',\n",
       "       \"Asking some his nationality is a Racial offence. Wow wasn't aware of it.  Blocking me has shown your support towards your community. Thanku for that\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[813, 4855, 200, 6773, 40, 7, 162, 8, 393, 234, 19, 20, 350, 14246],\n",
       " [46,\n",
       "  33,\n",
       "  14,\n",
       "  673,\n",
       "  114,\n",
       "  17,\n",
       "  6,\n",
       "  87,\n",
       "  21,\n",
       "  13,\n",
       "  71,\n",
       "  2,\n",
       "  917,\n",
       "  366,\n",
       "  22,\n",
       "  6,\n",
       "  283,\n",
       "  2,\n",
       "  33,\n",
       "  38,\n",
       "  6,\n",
       "  44,\n",
       "  16,\n",
       "  170,\n",
       "  31,\n",
       "  121],\n",
       " [732,\n",
       "  3,\n",
       "  504,\n",
       "  7,\n",
       "  184,\n",
       "  1,\n",
       "  732,\n",
       "  3,\n",
       "  504,\n",
       "  125,\n",
       "  6,\n",
       "  180,\n",
       "  68,\n",
       "  11,\n",
       "  1682,\n",
       "  449,\n",
       "  3,\n",
       "  13293,\n",
       "  7,\n",
       "  69,\n",
       "  6,\n",
       "  300,\n",
       "  239,\n",
       "  282,\n",
       "  2,\n",
       "  1,\n",
       "  400,\n",
       "  26,\n",
       "  711,\n",
       "  5,\n",
       "  148,\n",
       "  3,\n",
       "  504,\n",
       "  5369,\n",
       "  2,\n",
       "  16,\n",
       "  270,\n",
       "  4,\n",
       "  378,\n",
       "  1087,\n",
       "  2,\n",
       "  30,\n",
       "  740,\n",
       "  900,\n",
       "  210,\n",
       "  50,\n",
       "  29,\n",
       "  320,\n",
       "  244,\n",
       "  22,\n",
       "  6,\n",
       "  108,\n",
       "  2,\n",
       "  676,\n",
       "  50,\n",
       "  206,\n",
       "  20,\n",
       "  676,\n",
       "  64,\n",
       "  4,\n",
       "  150,\n",
       "  4707,\n",
       "  15,\n",
       "  29,\n",
       "  1935],\n",
       " [896,\n",
       "  61,\n",
       "  66,\n",
       "  2481,\n",
       "  8,\n",
       "  5,\n",
       "  2547,\n",
       "  4277,\n",
       "  1414,\n",
       "  594,\n",
       "  709,\n",
       "  3,\n",
       "  11,\n",
       "  836,\n",
       "  35,\n",
       "  43,\n",
       "  1172,\n",
       "  20,\n",
       "  358,\n",
       "  1037,\n",
       "  20,\n",
       "  486,\n",
       "  12,\n",
       "  9]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenized_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_rnn():\n",
    "    embed_size = 256\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    main = Embedding(max_features, embed_size)(inp)\n",
    "    main = Dropout(0.2)(main)\n",
    "#     main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "#     main = MaxPooling1D(pool_size=2)(main)\n",
    "#     main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "#     main = MaxPooling1D(pool_size=2)(main)\n",
    "    main = GRU(32)(main)\n",
    "#     main = Dense(16, activation=\"relu\")(main)\n",
    "    main = Dense(6, activation=\"sigmoid\")(main)\n",
    "    model = Model(inputs=inp, outputs=main)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Labels \n",
      "0    86061\n",
      "1     3833\n",
      "3     2523\n",
      "2     2107\n",
      "4     1076\n",
      "5      231\n",
      "6       20\n",
      "dtype: int64\n",
      "Training: (76680, 100)\n",
      "Testing: (19171, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Positive Labels ')\n",
    "any_category_positive = np.sum(y,1)\n",
    "print(pd.value_counts(any_category_positive))\n",
    "\n",
    "X_t_train, X_t_test, y_train, y_test = train_test_split(X_t, y,test_size = 0.20)\n",
    "print('Training:', X_t_train.shape)\n",
    "print('Testing:', X_t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_rnn()\n",
    "\n",
    "file_path=\"model_best.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76680 samples, validate on 19171 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.05307, saving model to model_best.h5\n",
      "272s - loss: 0.0793 - acc: 0.9762 - val_loss: 0.0531 - val_acc: 0.9808\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.05307 to 0.05071, saving model to model_best.h5\n",
      "305s - loss: 0.0456 - acc: 0.9834 - val_loss: 0.0507 - val_acc: 0.9817\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "307s - loss: 0.0370 - acc: 0.9861 - val_loss: 0.0518 - val_acc: 0.9817\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "313s - loss: 0.0293 - acc: 0.9890 - val_loss: 0.0589 - val_acc: 0.9811\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "306s - loss: 0.0228 - acc: 0.9917 - val_loss: 0.0633 - val_acc: 0.9804\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "306s - loss: 0.0179 - acc: 0.9935 - val_loss: 0.0711 - val_acc: 0.9797\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "306s - loss: 0.0137 - acc: 0.9952 - val_loss: 0.0824 - val_acc: 0.9791\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "308s - loss: 0.0109 - acc: 0.9962 - val_loss: 0.0895 - val_acc: 0.9781\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "308s - loss: 0.0088 - acc: 0.9969 - val_loss: 0.0972 - val_acc: 0.9788\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "309s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.1037 - val_acc: 0.9781\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "310s - loss: 0.0060 - acc: 0.9980 - val_loss: 0.1108 - val_acc: 0.9784\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "311s - loss: 0.0050 - acc: 0.9984 - val_loss: 0.1174 - val_acc: 0.9776\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "311s - loss: 0.0045 - acc: 0.9985 - val_loss: 0.1250 - val_acc: 0.9783\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "312s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.1280 - val_acc: 0.9768\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "317s - loss: 0.0035 - acc: 0.9988 - val_loss: 0.1287 - val_acc: 0.9773\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "318s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.1323 - val_acc: 0.9772\n",
      "Epoch 17/20\n",
      "Epoch 00016: val_loss did not improve\n",
      "312s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1404 - val_acc: 0.9762\n",
      "Epoch 18/20\n",
      "Epoch 00017: val_loss did not improve\n",
      "316s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.1437 - val_acc: 0.9749\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f292d28f8cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           verbose=2)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Whole_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python/python35/lib/python3.5/site-packages/Keras-2.0.8-py3.5.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/opt/python/python35/lib/python3.5/site-packages/Keras-2.0.8-py3.5.egg/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python/python35/lib/python3.5/site-packages/Keras-2.0.8-py3.5.egg/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python/python35/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python/python35/lib/python3.5/site-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_t_train, y_train,\n",
    "          validation_data=(X_t_test, y_test),\n",
    "          batch_size=50,\n",
    "          epochs=20,\n",
    "          shuffle = True,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=2)\n",
    "model.save('Whole_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(file_path)\n",
    "y_test = model.predict(X_te)\n",
    "sample_submission = pd.read_csv(path + \"sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(path + \"Keras_embedding_layer1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
