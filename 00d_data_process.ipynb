{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from dask.multiprocessing import get\n",
    "import datefinder\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "stm = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dates(text):\n",
    "    dates = list(datefinder.find_dates(text, source=True))\n",
    "    dates = [x[1] for x in dates]\n",
    "    for date in dates:\n",
    "        text = text.replace(date,'')\n",
    "    return text\n",
    "\n",
    "def clean_text(idx,text):\n",
    "    text = text.lower()\n",
    "    ipaddress = re.findall( r'[0-9]+(?:\\.[0-9]+){3}', text)\n",
    "    for ip in ipaddress:\n",
    "        text = text.replace(ip,'')\n",
    "    text = clean_dates(text)\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"havn't\", \"have not\")\n",
    "    text = text.replace(\"n't\",\" not\")\n",
    "    text = text.replace(\"i'm\", \"i am\")\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"there's\", \"there is\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    text = text.replace(\"e-mail\", \"email\")\n",
    "    text = text.replace(\"you'll\", \"you will\")\n",
    "    text = re.sub('([' + string.punctuation + '“”¨«»®´·º½¾¿¡§£₤‘’])', '', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = ' '.join([x.strip() for x in text])\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return {idx:text.strip().lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "text = ['id','comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (159571, 8)\n",
      "train data: (159571, 8)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/download/train.csv').fillna('nan')\n",
    "print('train data:', train_data.shape)\n",
    "train_labels = train_data[labels].drop_duplicates()\n",
    "train_text = train_data[text]\n",
    "train_text = dd.from_pandas(train_text, npartitions=10)\n",
    "train_text = train_text.map_partitions(lambda df: df.apply((lambda row: clean_text(*row)),axis=1))\n",
    "train_text = train_text.compute(get=get)\n",
    "train_text = pd.DataFrame(train_text, columns=['comment_text'])\n",
    "train_text['id'] = train_text['comment_text'].map(lambda x : list(x.items())[0][0])\n",
    "train_text['comment_text'] = train_text['comment_text'].map(lambda x : list(x.items())[0][1])\n",
    "train_data = train_text.merge(train_labels, on='id')\n",
    "print('train data:', train_data.shape)\n",
    "del train_text, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('../data/download/test.csv').fillna('nan')\n",
    "test_text = dd.from_pandas(test_data, npartitions=10)\n",
    "test_text = test_text.map_partitions(lambda df: df.apply((lambda row: clean_text(*row)),axis=1))\n",
    "test_text = test_text.compute(get=get)\n",
    "test_text = pd.DataFrame(test_text, columns=['comment_text'])\n",
    "test_text['id'] = test_text['comment_text'].map(lambda x : list(x.items())[0][0])\n",
    "test_text['comment_text'] = test_text['comment_text'].map(lambda x : list(x.items())[0][1])\n",
    "test_data = test_text[['id','comment_text']].copy()\n",
    "print('train data:', test_data.shape)\n",
    "del test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (159571, 15) (159571, 8)\n",
      "score data: (153164, 9) (153164, 8)\n"
     ]
    }
   ],
   "source": [
    "train_feats = pd.read_csv('../data/download/train_feats.csv')\n",
    "test_feats = pd.read_csv('../data/download/test_feats.csv')\n",
    "train_data = train_data.merge(train_feats, on='id')\n",
    "test_data = test_data.merge(test_feats, on='id')\n",
    "print('train data:', train_data.shape, train_feats.shape)\n",
    "print('score data:', test_data.shape, test_feats.shape)\n",
    "feats = text + list(train_feats.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = train_data['id'].unique()\n",
    "X = train_data[feats]\n",
    "y = train_data[labels]\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (127656, 9) (127656, 7)\n",
      "test data: (31915, 9) (31915, 7)\n",
      "train data: (127657, 9) (127657, 7)\n",
      "test data: (31914, 9) (31914, 7)\n",
      "train data: (127657, 9) (127657, 7)\n",
      "test data: (31914, 9) (31914, 7)\n",
      "train data: (127657, 9) (127657, 7)\n",
      "test data: (31914, 9) (31914, 7)\n",
      "train data: (127657, 9) (127657, 7)\n",
      "test data: (31914, 9) (31914, 7)\n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "source = 'source_1'\n",
    "\n",
    "for train_idx, test_idx in folds.split(comments):\n",
    "    train_idx = comments[train_idx]\n",
    "    test_idx = comments[test_idx]\n",
    "    X_train = X[X['id'].isin(train_idx)]\n",
    "    y_train = y[y['id'].isin(train_idx)]\n",
    "    X_test = X[X['id'].isin(test_idx)]\n",
    "    y_test = y[y['id'].isin(test_idx)]\n",
    "    print('train data:', X_train.shape, y_train.shape)\n",
    "    print('test data:', X_test.shape, y_test.shape)\n",
    "    path = '../data/data/{}/train/'.format(source)\n",
    "    X_train.to_csv(path + 'train_data_{}.csv'.format(fold), index=False)\n",
    "    y_train.to_csv(path + 'train_labels_{}.csv'.format(fold), index=False)\n",
    "    X_test.to_csv(path + 'test_data_{}.csv'.format(fold), index=False)\n",
    "    y_test.to_csv(path + 'test_labels_{}.csv'.format(fold), index=False)\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data: (153164, 9)\n"
     ]
    }
   ],
   "source": [
    "source = 'source_1'\n",
    "path = '../data/data/{}/score/'.format(source)\n",
    "test_data[feats].to_csv(path + 'score_text.csv', index=False)\n",
    "print('test data:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
