{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.layers import Dense, Dropout, GRU, Embedding \n",
    "from keras.layers import Input, Activation, concatenate, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils, get_custom_objects\n",
    "from keras.preprocessing import text, sequence\n",
    "from string import ascii_letters, punctuation, digits\n",
    "\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "SEQ_LENGTH = 100\n",
    "EMBED_SIZE = 100\n",
    "VOCAB = 238590\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(matrix):\n",
    "    rnn = {}\n",
    "    rnn['units'] = 50\n",
    "    rnn['return_sequences'] = True\n",
    "    rnn['recurrent_dropout'] = 0.2\n",
    "    rnn['dropout'] = 0.1\n",
    "    rnn['activation'] = 'tanh'\n",
    "    inputs = Input(shape=(SEQ_LENGTH,), name='sequence')\n",
    "    embed = Embedding(VOCAB,EMBED_SIZE, weights=[matrix], trainable=False)(inputs)\n",
    "    lstm = Bidirectional(GRU(**rnn))(embed)\n",
    "    lstm = BatchNormalization()(lstm)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm)\n",
    "    pool = concatenate([max_pool, avg_pool])\n",
    "    pool = BatchNormalization()(pool)\n",
    "    lstm = Dropout(0.2)(pool)\n",
    "    dense = Dense(200, activation='swish')(lstm)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    predict = Dense(6, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=[inputs], output=predict)\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93651 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('../data/data/fasttext/vector.vec')\n",
    "\n",
    "for line in f:\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(values[0])\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataflow(train_text, valid_text):\n",
    "    train_text['comment_text'] = train_text['comment_text'].fillna('nan')\n",
    "    valid_text['comment_text'] = valid_text['comment_text'].fillna('nan')\n",
    "    train_text = list(train_text['comment_text'].values)\n",
    "    valid_text = list(valid_text['comment_text'].values)\n",
    "    tokenizer = text.Tokenizer(lower=True, char_level=False, num_words=100000)\n",
    "    tokenizer.fit_on_texts(train_text + valid_text)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('total words:', len(word_index))\n",
    "    intersect = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            intersect += 1\n",
    "    print('common words:', intersect)\n",
    "    train_token = tokenizer.texts_to_sequences(train_text)\n",
    "    valid_token = tokenizer.texts_to_sequences(valid_text)\n",
    "    train_seq = sequence.pad_sequences(train_token, maxlen=SEQ_LENGTH)\n",
    "    valid_seq = sequence.pad_sequences(valid_token, maxlen=SEQ_LENGTH)\n",
    "    return train_seq, valid_seq, embedding_matrix\n",
    "\n",
    "def callbacks(suffix):\n",
    "    stop = EarlyStopping('val_loss', patience=5, mode=\"min\")\n",
    "    path = '../data/data/source_1/model_1/model_{}.hdf5'.format(suffix)\n",
    "    save = ModelCheckpoint(path, save_best_only=True, save_weights_only=True)\n",
    "    logger = CSVLogger('../data/data/source_1/model_1/logger_{}.log'.format(suffix))\n",
    "    reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=2, verbose=0, mode='min')\n",
    "    return [stop, save, reduce, logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data : fold - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 238589\n",
      "common words: 88118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/30\n",
      "127656/127656 [==============================] - 205s 2ms/step - loss: 0.0805 - acc: 0.9724 - val_loss: 0.0500 - val_acc: 0.9815\n",
      "Epoch 2/30\n",
      "127656/127656 [==============================] - 202s 2ms/step - loss: 0.0517 - acc: 0.9809 - val_loss: 0.0440 - val_acc: 0.9831\n",
      "Epoch 3/30\n",
      "127656/127656 [==============================] - 222s 2ms/step - loss: 0.0487 - acc: 0.9818 - val_loss: 0.0430 - val_acc: 0.9835\n",
      "Epoch 4/30\n",
      "127656/127656 [==============================] - 205s 2ms/step - loss: 0.0463 - acc: 0.9826 - val_loss: 0.0424 - val_acc: 0.9833\n",
      "Epoch 5/30\n",
      "127656/127656 [==============================] - 205s 2ms/step - loss: 0.0450 - acc: 0.9829 - val_loss: 0.0420 - val_acc: 0.9838\n",
      "Epoch 6/30\n",
      "127656/127656 [==============================] - 224s 2ms/step - loss: 0.0442 - acc: 0.9832 - val_loss: 0.0419 - val_acc: 0.9835\n",
      "Epoch 7/30\n",
      "127656/127656 [==============================] - 223s 2ms/step - loss: 0.0435 - acc: 0.9833 - val_loss: 0.0418 - val_acc: 0.9836\n",
      "Epoch 8/30\n",
      "127656/127656 [==============================] - 222s 2ms/step - loss: 0.0427 - acc: 0.9836 - val_loss: 0.0418 - val_acc: 0.9835\n",
      "Epoch 9/30\n",
      "127656/127656 [==============================] - 241s 2ms/step - loss: 0.0421 - acc: 0.9837 - val_loss: 0.0415 - val_acc: 0.9841\n",
      "Epoch 10/30\n",
      "127656/127656 [==============================] - 240s 2ms/step - loss: 0.0417 - acc: 0.9838 - val_loss: 0.0410 - val_acc: 0.9839\n",
      "Epoch 11/30\n",
      "127656/127656 [==============================] - 228s 2ms/step - loss: 0.0412 - acc: 0.9839 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "Epoch 12/30\n",
      "127656/127656 [==============================] - 238s 2ms/step - loss: 0.0407 - acc: 0.9842 - val_loss: 0.0410 - val_acc: 0.9840\n",
      "Epoch 13/30\n",
      "127656/127656 [==============================] - 236s 2ms/step - loss: 0.0401 - acc: 0.9841 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 14/30\n",
      "127656/127656 [==============================] - 230s 2ms/step - loss: 0.0400 - acc: 0.9842 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 15/30\n",
      "127656/127656 [==============================] - 190s 1ms/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0422 - val_acc: 0.9833\n",
      "Epoch 16/30\n",
      "127656/127656 [==============================] - 143s 1ms/step - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0414 - val_acc: 0.9836\n",
      "Epoch 17/30\n",
      "127656/127656 [==============================] - 184s 1ms/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0408 - val_acc: 0.9840\n",
      "Epoch 18/30\n",
      "127656/127656 [==============================] - 191s 1ms/step - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 19/30\n",
      "127656/127656 [==============================] - 184s 1ms/step - loss: 0.0369 - acc: 0.9854 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 20/30\n",
      "127656/127656 [==============================] - 192s 2ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 21/30\n",
      "127656/127656 [==============================] - 185s 1ms/step - loss: 0.0366 - acc: 0.9854 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 22/30\n",
      "127656/127656 [==============================] - 189s 1ms/step - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 23/30\n",
      "127656/127656 [==============================] - 192s 2ms/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 24/30\n",
      "127656/127656 [==============================] - 190s 1ms/step - loss: 0.0365 - acc: 0.9854 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 25/30\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.0364 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 26/30\n",
      "127656/127656 [==============================] - 214s 2ms/step - loss: 0.0366 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 27/30\n",
      "127656/127656 [==============================] - 190s 1ms/step - loss: 0.0365 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 28/30\n",
      "127656/127656 [==============================] - 208s 2ms/step - loss: 0.0364 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 29/30\n",
      "127656/127656 [==============================] - 191s 1ms/step - loss: 0.0363 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 30/30\n",
      "127656/127656 [==============================] - 208s 2ms/step - loss: 0.0364 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 0.9843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe949438e48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = pd.read_csv('../data/data/source_1/train/train_data_1.csv')\n",
    "train_label = pd.read_csv('../data/data/source_1/train/train_labels_1.csv')\n",
    "valid_text = pd.read_csv('../data/data/source_1/train/test_data_1.csv')\n",
    "valid_label = pd.read_csv('../data/data/source_1/train/test_labels_1.csv')\n",
    "train_text, valid_text, embedding_matrix = dataflow(train_text, valid_text)\n",
    "\n",
    "params = {}\n",
    "params['x'] = train_text\n",
    "params['y'] = np.array(train_label.iloc[:,1:])\n",
    "params['validation_data'] = (valid_text, np.array(valid_label.iloc[:,1:]))\n",
    "params['batch_size'] = 256\n",
    "params['epochs'] = 30\n",
    "params['verbose'] = 1\n",
    "params['callbacks'] = callbacks(1)\n",
    "model = define_model(embedding_matrix)\n",
    "model.fit(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data : fold - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 238589\n",
      "common words: 88118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/30\n",
      "127657/127657 [==============================] - 212s 2ms/step - loss: 0.0817 - acc: 0.9719 - val_loss: 0.0564 - val_acc: 0.9792\n",
      "Epoch 2/30\n",
      "127657/127657 [==============================] - 211s 2ms/step - loss: 0.0515 - acc: 0.9811 - val_loss: 0.0454 - val_acc: 0.9829\n",
      "Epoch 3/30\n",
      "127657/127657 [==============================] - 207s 2ms/step - loss: 0.0479 - acc: 0.9820 - val_loss: 0.0463 - val_acc: 0.9816\n",
      "Epoch 4/30\n",
      "127657/127657 [==============================] - 199s 2ms/step - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0433 - val_acc: 0.9835\n",
      "Epoch 5/30\n",
      "127657/127657 [==============================] - 216s 2ms/step - loss: 0.0449 - acc: 0.9828 - val_loss: 0.0430 - val_acc: 0.9835\n",
      "Epoch 6/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0437 - acc: 0.9833 - val_loss: 0.0433 - val_acc: 0.9836\n",
      "Epoch 7/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0430 - acc: 0.9835 - val_loss: 0.0428 - val_acc: 0.9834\n",
      "Epoch 8/30\n",
      "127657/127657 [==============================] - 217s 2ms/step - loss: 0.0423 - acc: 0.9837 - val_loss: 0.0428 - val_acc: 0.9835\n",
      "Epoch 9/30\n",
      "127657/127657 [==============================] - 213s 2ms/step - loss: 0.0418 - acc: 0.9839 - val_loss: 0.0429 - val_acc: 0.9834\n",
      "Epoch 10/30\n",
      "127657/127657 [==============================] - 229s 2ms/step - loss: 0.0410 - acc: 0.9840 - val_loss: 0.0426 - val_acc: 0.9836\n",
      "Epoch 11/30\n",
      "127657/127657 [==============================] - 224s 2ms/step - loss: 0.0406 - acc: 0.9842 - val_loss: 0.0430 - val_acc: 0.9830\n",
      "Epoch 12/30\n",
      "127657/127657 [==============================] - 201s 2ms/step - loss: 0.0401 - acc: 0.9843 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "Epoch 13/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0396 - acc: 0.9844 - val_loss: 0.0426 - val_acc: 0.9836\n",
      "Epoch 14/30\n",
      "127657/127657 [==============================] - 231s 2ms/step - loss: 0.0382 - acc: 0.9849 - val_loss: 0.0424 - val_acc: 0.9839\n",
      "Epoch 15/30\n",
      "127657/127657 [==============================] - 228s 2ms/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0421 - val_acc: 0.9840\n",
      "Epoch 16/30\n",
      "127657/127657 [==============================] - 225s 2ms/step - loss: 0.0375 - acc: 0.9852 - val_loss: 0.0423 - val_acc: 0.9839\n",
      "Epoch 17/30\n",
      "127657/127657 [==============================] - 211s 2ms/step - loss: 0.0376 - acc: 0.9851 - val_loss: 0.0421 - val_acc: 0.9839\n",
      "Epoch 18/30\n",
      "127657/127657 [==============================] - 187s 1ms/step - loss: 0.0370 - acc: 0.9853 - val_loss: 0.0424 - val_acc: 0.9839\n",
      "Epoch 19/30\n",
      "127657/127657 [==============================] - 207s 2ms/step - loss: 0.0369 - acc: 0.9853 - val_loss: 0.0422 - val_acc: 0.9840\n",
      "Epoch 20/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0422 - val_acc: 0.9840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe93319ae80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = pd.read_csv('../data/data/source_1/train/train_data_2.csv')\n",
    "train_label = pd.read_csv('../data/data/source_1/train/train_labels_2.csv')\n",
    "valid_text = pd.read_csv('../data/data/source_1/train/test_data_2.csv')\n",
    "valid_label = pd.read_csv('../data/data/source_1/train/test_labels_2.csv')\n",
    "train_text, valid_text, embedding_matrix = dataflow(train_text, valid_text)\n",
    "\n",
    "params = {}\n",
    "params['x'] = train_text\n",
    "params['y'] = np.array(train_label.iloc[:,1:])\n",
    "params['validation_data'] = (valid_text, np.array(valid_label.iloc[:,1:]))\n",
    "params['batch_size'] = 256\n",
    "params['epochs'] = 30\n",
    "params['verbose'] = 1\n",
    "params['callbacks'] = callbacks(2)\n",
    "model = define_model(embedding_matrix)\n",
    "model.fit(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data : fold - 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 238589\n",
      "common words: 88118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/30\n",
      "127657/127657 [==============================] - 195s 2ms/step - loss: 0.0787 - acc: 0.9731 - val_loss: 0.0525 - val_acc: 0.9806\n",
      "Epoch 2/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0513 - acc: 0.9809 - val_loss: 0.0455 - val_acc: 0.9831\n",
      "Epoch 3/30\n",
      "127657/127657 [==============================] - 215s 2ms/step - loss: 0.0481 - acc: 0.9819 - val_loss: 0.0448 - val_acc: 0.9832\n",
      "Epoch 4/30\n",
      "127657/127657 [==============================] - 199s 2ms/step - loss: 0.0464 - acc: 0.9823 - val_loss: 0.0439 - val_acc: 0.9833\n",
      "Epoch 5/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0451 - acc: 0.9827 - val_loss: 0.0439 - val_acc: 0.9832\n",
      "Epoch 6/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0439 - acc: 0.9831 - val_loss: 0.0433 - val_acc: 0.9836\n",
      "Epoch 7/30\n",
      "127657/127657 [==============================] - 200s 2ms/step - loss: 0.0431 - acc: 0.9834 - val_loss: 0.0430 - val_acc: 0.9835\n",
      "Epoch 8/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0424 - acc: 0.9836 - val_loss: 0.0433 - val_acc: 0.9833\n",
      "Epoch 9/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0420 - acc: 0.9838 - val_loss: 0.0429 - val_acc: 0.9836\n",
      "Epoch 10/30\n",
      "127657/127657 [==============================] - 208s 2ms/step - loss: 0.0413 - acc: 0.9838 - val_loss: 0.0448 - val_acc: 0.9826\n",
      "Epoch 11/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0407 - acc: 0.9841 - val_loss: 0.0430 - val_acc: 0.9837\n",
      "Epoch 12/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0401 - acc: 0.9844 - val_loss: 0.0433 - val_acc: 0.9834\n",
      "Epoch 13/30\n",
      "127657/127657 [==============================] - 231s 2ms/step - loss: 0.0385 - acc: 0.9848 - val_loss: 0.0424 - val_acc: 0.9837\n",
      "Epoch 14/30\n",
      "127657/127657 [==============================] - 224s 2ms/step - loss: 0.0384 - acc: 0.9848 - val_loss: 0.0425 - val_acc: 0.9838\n",
      "Epoch 15/30\n",
      "127657/127657 [==============================] - 203s 2ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0424 - val_acc: 0.9837\n",
      "Epoch 16/30\n",
      "127657/127657 [==============================] - 223s 2ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0425 - val_acc: 0.9837\n",
      "Epoch 17/30\n",
      "127657/127657 [==============================] - 224s 2ms/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "Epoch 18/30\n",
      "127657/127657 [==============================] - 210s 2ms/step - loss: 0.0374 - acc: 0.9851 - val_loss: 0.0424 - val_acc: 0.9839\n",
      "Epoch 19/30\n",
      "127657/127657 [==============================] - 232s 2ms/step - loss: 0.0372 - acc: 0.9853 - val_loss: 0.0423 - val_acc: 0.9839\n",
      "Epoch 20/30\n",
      "127657/127657 [==============================] - 230s 2ms/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0423 - val_acc: 0.9838\n",
      "Epoch 21/30\n",
      "127657/127657 [==============================] - 183s 1ms/step - loss: 0.0372 - acc: 0.9853 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "Epoch 22/30\n",
      "127657/127657 [==============================] - 208s 2ms/step - loss: 0.0372 - acc: 0.9853 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "Epoch 23/30\n",
      "127657/127657 [==============================] - 207s 2ms/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "Epoch 24/30\n",
      "127657/127657 [==============================] - 193s 2ms/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0423 - val_acc: 0.9838\n",
      "Epoch 25/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0374 - acc: 0.9853 - val_loss: 0.0423 - val_acc: 0.9838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe932ec4ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = pd.read_csv('../data/data/source_1/train/train_data_3.csv')\n",
    "train_label = pd.read_csv('../data/data/source_1/train/train_labels_3.csv')\n",
    "valid_text = pd.read_csv('../data/data/source_1/train/test_data_3.csv')\n",
    "valid_label = pd.read_csv('../data/data/source_1/train/test_labels_3.csv')\n",
    "train_text, valid_text, embedding_matrix = dataflow(train_text, valid_text)\n",
    "\n",
    "params = {}\n",
    "params['x'] = train_text\n",
    "params['y'] = np.array(train_label.iloc[:,1:])\n",
    "params['validation_data'] = (valid_text, np.array(valid_label.iloc[:,1:]))\n",
    "params['batch_size'] = 256\n",
    "params['epochs'] = 30\n",
    "params['verbose'] = 1\n",
    "params['callbacks'] = callbacks(3)\n",
    "model = define_model(embedding_matrix)\n",
    "model.fit(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data : fold - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 238589\n",
      "common words: 88118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/30\n",
      "127657/127657 [==============================] - 196s 2ms/step - loss: 0.0814 - acc: 0.9718 - val_loss: 0.0476 - val_acc: 0.9821\n",
      "Epoch 2/30\n",
      "127657/127657 [==============================] - 206s 2ms/step - loss: 0.0516 - acc: 0.9810 - val_loss: 0.0451 - val_acc: 0.9826\n",
      "Epoch 3/30\n",
      "127657/127657 [==============================] - 215s 2ms/step - loss: 0.0483 - acc: 0.9821 - val_loss: 0.0438 - val_acc: 0.9830\n",
      "Epoch 4/30\n",
      "127657/127657 [==============================] - 208s 2ms/step - loss: 0.0466 - acc: 0.9825 - val_loss: 0.0437 - val_acc: 0.9832\n",
      "Epoch 5/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0450 - acc: 0.9829 - val_loss: 0.0445 - val_acc: 0.9825\n",
      "Epoch 6/30\n",
      "127657/127657 [==============================] - 200s 2ms/step - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0429 - val_acc: 0.9834\n",
      "Epoch 7/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0435 - acc: 0.9834 - val_loss: 0.0421 - val_acc: 0.9836\n",
      "Epoch 8/30\n",
      "127657/127657 [==============================] - 216s 2ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 9/30\n",
      "127657/127657 [==============================] - 193s 2ms/step - loss: 0.0419 - acc: 0.9839 - val_loss: 0.0422 - val_acc: 0.9837\n",
      "Epoch 10/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0413 - acc: 0.9839 - val_loss: 0.0420 - val_acc: 0.9837\n",
      "Epoch 11/30\n",
      "127657/127657 [==============================] - 208s 2ms/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0421 - val_acc: 0.9833\n",
      "Epoch 12/30\n",
      "127657/127657 [==============================] - 213s 2ms/step - loss: 0.0394 - acc: 0.9846 - val_loss: 0.0419 - val_acc: 0.9836\n",
      "Epoch 13/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0416 - val_acc: 0.9837\n",
      "Epoch 14/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0417 - val_acc: 0.9838\n",
      "Epoch 15/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0417 - val_acc: 0.9838\n",
      "Epoch 16/30\n",
      "127657/127657 [==============================] - 222s 2ms/step - loss: 0.0386 - acc: 0.9846 - val_loss: 0.0416 - val_acc: 0.9837\n",
      "Epoch 17/30\n",
      "127657/127657 [==============================] - 231s 2ms/step - loss: 0.0381 - acc: 0.9849 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 18/30\n",
      "127657/127657 [==============================] - 232s 2ms/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0415 - val_acc: 0.9839\n",
      "Epoch 19/30\n",
      "127657/127657 [==============================] - 222s 2ms/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 20/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0416 - val_acc: 0.9839\n",
      "Epoch 21/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0376 - acc: 0.9851 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 22/30\n",
      "127657/127657 [==============================] - 187s 1ms/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0416 - val_acc: 0.9839\n",
      "Epoch 23/30\n",
      "127657/127657 [==============================] - 208s 2ms/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0416 - val_acc: 0.9838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe92a049eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = pd.read_csv('../data/data/source_1/train/train_data_4.csv')\n",
    "train_label = pd.read_csv('../data/data/source_1/train/train_labels_4.csv')\n",
    "valid_text = pd.read_csv('../data/data/source_1/train/test_data_4.csv')\n",
    "valid_label = pd.read_csv('../data/data/source_1/train/test_labels_4.csv')\n",
    "train_text, valid_text, embedding_matrix = dataflow(train_text, valid_text)\n",
    "\n",
    "params = {}\n",
    "params['x'] = train_text\n",
    "params['y'] = np.array(train_label.iloc[:,1:])\n",
    "params['validation_data'] = (valid_text, np.array(valid_label.iloc[:,1:]))\n",
    "params['batch_size'] = 256\n",
    "params['epochs'] = 30\n",
    "params['verbose'] = 1\n",
    "params['callbacks'] = callbacks(4)\n",
    "model = define_model(embedding_matrix)\n",
    "model.fit(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data : fold - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 238589\n",
      "common words: 88118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0818 - acc: 0.9718 - val_loss: 0.0516 - val_acc: 0.9809\n",
      "Epoch 2/30\n",
      "127657/127657 [==============================] - 198s 2ms/step - loss: 0.0510 - acc: 0.9813 - val_loss: 0.0494 - val_acc: 0.9809\n",
      "Epoch 3/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0479 - acc: 0.9819 - val_loss: 0.0465 - val_acc: 0.9823\n",
      "Epoch 4/30\n",
      "127657/127657 [==============================] - 201s 2ms/step - loss: 0.0458 - acc: 0.9826 - val_loss: 0.0457 - val_acc: 0.9829\n",
      "Epoch 5/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0447 - acc: 0.9828 - val_loss: 0.0449 - val_acc: 0.9828\n",
      "Epoch 6/30\n",
      "127657/127657 [==============================] - 200s 2ms/step - loss: 0.0434 - acc: 0.9834 - val_loss: 0.0445 - val_acc: 0.9831\n",
      "Epoch 7/30\n",
      "127657/127657 [==============================] - 208s 2ms/step - loss: 0.0426 - acc: 0.9835 - val_loss: 0.0449 - val_acc: 0.9829\n",
      "Epoch 8/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0419 - acc: 0.9838 - val_loss: 0.0448 - val_acc: 0.9830\n",
      "Epoch 9/30\n",
      "127657/127657 [==============================] - 214s 2ms/step - loss: 0.0415 - acc: 0.9839 - val_loss: 0.0445 - val_acc: 0.9832\n",
      "Epoch 10/30\n",
      "127657/127657 [==============================] - 205s 2ms/step - loss: 0.0398 - acc: 0.9844 - val_loss: 0.0439 - val_acc: 0.9835\n",
      "Epoch 11/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0394 - acc: 0.9845 - val_loss: 0.0440 - val_acc: 0.9834\n",
      "Epoch 12/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0394 - acc: 0.9845 - val_loss: 0.0441 - val_acc: 0.9834\n",
      "Epoch 13/30\n",
      "127657/127657 [==============================] - 211s 2ms/step - loss: 0.0390 - acc: 0.9848 - val_loss: 0.0439 - val_acc: 0.9836\n",
      "Epoch 14/30\n",
      "127657/127657 [==============================] - 231s 2ms/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0439 - val_acc: 0.9834\n",
      "Epoch 15/30\n",
      "127657/127657 [==============================] - 225s 2ms/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0439 - val_acc: 0.9834\n",
      "Epoch 16/30\n",
      "127657/127657 [==============================] - 209s 2ms/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0439 - val_acc: 0.9834\n",
      "Epoch 17/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0382 - acc: 0.9849 - val_loss: 0.0439 - val_acc: 0.9834\n",
      "Epoch 18/30\n",
      "127657/127657 [==============================] - 226s 2ms/step - loss: 0.0383 - acc: 0.9851 - val_loss: 0.0439 - val_acc: 0.9833\n",
      "Epoch 19/30\n",
      "127657/127657 [==============================] - 187s 1ms/step - loss: 0.0383 - acc: 0.9848 - val_loss: 0.0439 - val_acc: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8999585c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = pd.read_csv('../data/data/source_1/train/train_data_5.csv')\n",
    "train_label = pd.read_csv('../data/data/source_1/train/train_labels_5.csv')\n",
    "valid_text = pd.read_csv('../data/data/source_1/train/test_data_5.csv')\n",
    "valid_label = pd.read_csv('../data/data/source_1/train/test_labels_5.csv')\n",
    "train_text, valid_text, embedding_matrix = dataflow(train_text, valid_text)\n",
    "\n",
    "params = {}\n",
    "params['x'] = train_text\n",
    "params['y'] = np.array(train_label.iloc[:,1:])\n",
    "params['validation_data'] = (valid_text, np.array(valid_label.iloc[:,1:]))\n",
    "params['batch_size'] = 256\n",
    "params['epochs'] = 30\n",
    "params['verbose'] = 1\n",
    "params['callbacks'] = callbacks(5)\n",
    "model = define_model(embedding_matrix)\n",
    "model.fit(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
